1. Knowledge base 的 缺失性（从zipf定律上可以看出来）

2. DB与FB的非完全重合性

3. small network 需要经过700次的扩展才能完成20%的share node搜索。 但是真实的 fb15k 只需要两次就可以完成。 说明局部特性是相当明显的。

4. 问 : 我为什么这么执着于实现两个子空间的转换?

   答 : 因为这个问题里面存在着一个很本质, 也很重要的问题. 那就是如何利用局部的稳定性去增强模型. 

   我们如今的NLP模型都大多把精力放在特定的任务上, 但是由于数据量较少,  所以一直达不到理想的效果. 

   这个和我们的问题其实是很像的, 但是, 这个图结构的很重要的一点是, 其有局部稳定性. (需要证明:通过神经过程和简单神经网络的比较.)

   这个先验对于图数据的学习是非常有用的. 因为, 即使只有很少的数据, 我们也可以利用网络结构稳定性, 通过将网络结构中的少数监督数据下进行一个转换, 而实现整体的转换. 这就要求一点 : 图结构要具有及其稳定的结构. 

   > 就比如说, 我们有一个由球组成的网, 如果球的连接是很松的线, 那么我们将其换一个位置后, 整个网中各个球的相对位置就会变化, 根据一部分数据进行推理就变得无比困难, 甚至可以说是不可行. 那么, 要解决这个问题就有两种方法:
   >
   > - 学得一个稳固的结构, 比如说通过边和边之间的组合约束去限制其空间结构. 也就是说, 减小局部最优解. 在上面的例子中就是, 尽量让连接球的边结实, 最好是钢筋的. 
   > - 学得空间变化模型, 即使是用弹簧连在一起的, 我们只要能够学得其变化模式, 也就能够进行预测. 学得变化模式的方法有:
   >   - 以 TransE 为例, 设置多种初始化或者随机化方案, 学习到收敛后, 将学得的所有的embedding空间作为输入, 研究其相对应关系.  如果 模型(TransE) 够稳定, 那么只需要很少的几次训练, 便可找到转换的模式.
   >   - 但是, 这个方法要求进行编码的encoder本身具有确定性, 如果本身就有很多随机噪音的话, 只能通过大量训练模型, 然后根据大数据下的统计性质去找到最优转换模式. 
   >   - 这里需要一个对模型可信度进行评价的一个指标.

   而现在在 NLP task 的一个常见的方法是, 先进行预训练, 然后在特定数据集以及模型上进行训练, 那么这个地方就有一个问题, 这个和我knowledge graph integration 中的 GNN joint  模型是一样的, 也就是没有利用到语言知识本身的稳定性, 这也就是为什么在有些任务上直接使用强大词向量+简单模型就可以超越很多复杂模型的原因. 

   所以在NLP模型上考虑知识本身的稳定性是很重要的. (这个稳定性的存在需要我进行证明)

   Multi-task 和 Transfer learning 之所以有效果, 也就是用到了这一点. 他们将各个数据集或者各个模型之间的偏置性给中和掉了, 因此可以获得很好的效果. 





#### 接下来任务

- 证明 局部稳定性, 利用 neural process 中的架构去做. 暂时这里的 context encoder 先用简单的求平均模式.
- 找一个更好的 context encoder 以及加入 随机性进行测验.
- 在各种不同的 knowledge Embedding 结果, 探索上面提到的模型稳定性问题(也就是能够学到一个稳定的结构的问题)
- 按照自己的 penta TransE 模型进行测试. 
- 扩展到 NLP 任务.