## Regression 

## 0. Introduction

监督性学习可以分为回归和分类两种模型, 分别对应着连续形式和离散形式.

有很多中可以去解释GP(高斯过程)回归模型的方法, 而这里基于函数的观点 - function-view

分节内容如下:

- 2.1 先利用 weight-space 的观点去解释高斯过程, 便于初步理解.
- 2.2 用 function-view 去理解.
- 2.3 高斯过程的行为会随着超参数的变化而变化, 这里讲解参数如何影响高斯过程.以及预测分布相关知识.
- 2.4 讨论如何组合损失函数和预测分布去利用决策理论以最优的方式做出一个点预测. 
- 2.5 一个实际的例子
- 2.6 关于高斯过程的更多的理论分析.
- 2.7 如何整合显式的偏执方程到模型中
- 2.8 历史观点



## 1.  Weight-space View

### 1.0 Introduction

这一节

1. 先是从贝叶斯角度去分析线性模型.
2. 然后使用核技巧去将输入投影到高位空间, 从而对线性模型做出增强.
3. 最后讨论当输入空间非常大以至于多于数据点数时的解决方法.

Notation:

- 输入向量维度是 $D$
- 输入集 : $X$
- 输出集 : $\mathbb{y}$


- 数据集![](./pictures/1.png) ![](./pictures/2)

### 1.1  The Standard Linear Model

#### 1.1.1 概率视角的线性模型

从贝叶斯角度来看的线性模型:

![](./pictures/3)

其中误差遵循独立同分布的高斯分布:

![](./pictures/4)

注意, 这里引入的 **随机变量(noise) $\epsilon$ ** , 将观测值 $\mathbb{y}$ 也变成了一个　**在给定参数下的分布的形式**. 

#### 1.1.2 观测值的分布形式

根据观测值来求最大概率.

![](./pictures/5)

这里就是将1.1.1中提到的加入高斯分布的函数进行了概率化. 

> **函数形式+具体数据(理应为无限)->概率分布形式**

#### 1.1.3 贝叶斯视角的线性模型

##### 1). **贝叶斯公式**

贝叶斯视角即分布中的参数遵循某种分布.

![](./pictures/6)

> 这个地方我有个疑问, 为什么 w 的均值会是0? 他很大可能性是不会是零的, 否则, 预测值就该一直是0. 而这里采用0的意义是, 我们对数据一无所知! 因此采取最中立, 还有最小信息的0.

这个就是这个模型中的贝叶斯先验分布.

根据贝叶斯分布的公式, 得到后验概率:

![](./pictures/7)

##### 2). **分母是不变的**

其中分母的值是不变的

> 关于这个有两个解释:
>
> - 数据集是已经固定的了,因此是定植
> - 分母是和w无关的, 因为是积分.![](/home/dreamer/files/my-notebooks/books&courses/book:Gaussian%20Process%20for%20Machine%20Learning/pictures/8)

##### 3). **展开贝叶斯公式**

对后验公式展开, 并进行转换.

![](./pictures/9)

其中, ![](./pictures/10)

再次进行转换:

![](./pictures/11)

其中, ![](./pictures/12)

这样我们发现, 在 **假设先验为高斯分布的前提下, 我们得到了一个新的后验高斯分布.**

说明这个模型是共扼分布.

##### 4). **MAP以及岭回归**

MAP的意义是 : maximum a posteriori (MAP), 使用的是对数最大后验法.

- 我们这个时候回顾一般的最小二乘法求解线性模型的方法.

  $w=\underset{w}{\operatorname{argmin}}||y-w^Tx||^2$

- 再看看应用了贝叶斯的最大后验法:

  $w = \underset{w}{\operatorname{argmax}}\ p(w|X,y) =\underset{w}{\operatorname{argmax}}\ \left(-\large(||y-w^Tx||^2/2\sigma_n^2 +\frac{1}{2}w^T\sum_p^{-1}w\large)\right)\\=\underset{w}{\operatorname{argmin}}\ \large(||y-w^Tx||^2/2\sigma_n^2 +\frac{1}{2}w^T\sum_p^{-1}w\large)$

这样的话, 我们就看到, 相比于一般的最小二乘法, 这里只不过是加了一个 **惩罚项**, 但是其在这里的意义却是 **先验概率.**

不同于通过数学分析组织出来的 L2 惩罚项(具体分析见deep learning 笔记第七章). 这里通过贝叶斯方法的先验分布成功地顺其自然地加入了防止过拟合的惩罚项. 

这从一个侧面说明了 : **贝叶斯对与参数符合某一分布的设定, 成功了加入了噪声, 提高了模型的鲁帮性**

##### 5). 预测分布

上面一直说的是对w的分布. 但是我们最终要求的是在给定一个输入下, 输出值的分布. 也就是预测分布.

对于, 输入 $\mathbb{x}_*$, 我们假设其输出为 $f(x_*)=f_*$. 则有:

![](./pictures/13)

这里其使用贝叶斯公式去导更加方便一些.

**这个地方实在是太重要了!!!** 这也是为什么要用 **高斯分布做分布假设的原因**, 高斯分布对于很多函数经过转换之后得到的 **预测函数** 依然是 高斯分布.

这里面还有一个非常有意思的点:

预测分布的方差是一个关于输入的二项式函数, 说明, 随着输入的 magnitude 的增加, 预测也就逐渐不准确. 这个时候你可能会问了, 如果数据集本身就是在无零区域的话, 这个岂不是有悖常识, 但是不要忘了, 这里还有个A, ![](./pictures/12) A 在这里应该是可以对这个行为做出解释的. 而我, 并不知道怎么解释.^_^



### 1.2 Projections of Inputs into Feature Space

这里其实在贝叶斯公式推导部分和上面的没有区别, 只是做了一个将低位的输入映射到高维的操作而已, 用的是核方法. 这里不赘述核方法的概念.

下面是公式:

1. 核方法进行高维映射

![](./pictures/14)

2. 直接得出预测分布:

   ![](./pictures/15)

   其中, ![](./pictures/16)

ok, 完.



## 2. Function-space View

### 2.1 Introduction

- **定义**

  高斯过程是一个随机变量的集合, 其中任意数量的随机变量之间都遵循联合高斯分布.

  这里的随机变量应该指的是由输入数据得到的实际值, 即 $f(x)$ , 这里把每个 $f(x_i)$ 到当作一个随机变量. 

  均值函数和协方差函数的形式如下:

  ![](/home/dreamer/files/my-notebooks/books&courses/book:Gaussian%20Process%20for%20Machine%20Learning/pictures/17)

  模型为 :

  ![](/home/dreamer/files/my-notebooks/books&courses/book:Gaussian%20Process%20for%20Machine%20Learning/pictures/18)

  具体的求解过程见2.3.

- **有趣之处**

  这就是这个模型有意思的地方 : 将 x 与 y 之间的关系, 转换为了 $f(x)$ 之间的关系. 

  会想到这里, 我们会发现, 到目前为止我们都没有提任何和模型本身性质相关的事, 这里说模型本身的性质是指, 是线性回归, 还是指数回归, 还是其他什么的. 那是不是这个贝叶斯就是这么厉害不用你设定就可以自动发掘出其模式呢? 

  答, 不是的, 我们要**通过设定协方差方程的类型来设定模型的类型**, 具体下面会讲.

  在 weight-view 中, 我们是通过将 w为高斯分布的假设应用到式子中后, 展开式子得出了预测分布. 但是这里不一样, 这里没有尝试去直接推算出预测分布, 而是通过随机变量的均值和方差函数得到最后的值.

- **高斯过程的性质**

  - **consistency requirement**

    **又叫做边缘性质(marginalization property)**

    整体服从多维高斯分布，那么其中一部分也服从对应均值和协方差矩阵的正态分布。

    其实就是说, 若两个变量符合正太分布, 那么其中一个变量也符合正态分布. 公式为:

    $p(x) = \int p(x,y)dy=\int \mathcal{N}(\mu,\sum)dy=\mathcal{N}(\mu_x,\sum_x)$

    这个性质很重要的一点是, 在大的随机变量集的测试结果不会影响小的随机变量集上的性质.

    > 这里的随机变量集是指 : 这里可以理解为训练数据集, 因为高斯过程中的随机变量就是数据集中的数据

  - **满足 consistency requirement的条件**

    条件是, 定义协方差矩阵的函数必须能决定协方差矩阵的所有条目.

    > 这里的条目(entries)是矩阵中的内容的意思, 比如对于 k*k 的协方差矩阵,  (i,j)- th entry of covariance matrix 代表的是变量 i和j 之间的协方差值.

### 2.2 线性模型的均值和方差

还是用刚才的使用核方法的例子来解释. 即, weight-view 是 

![](./pictures/19)

其中, $\mathcal{\phi}(x)=(x,x^2,x^3,...)$

那么对于两个具体的数据点 $x,x'$, 我们可以计算其 mean 和 协方差:

- **mean**

  单均值和联合均值:

  ![](./pictures/20)

- **variance**

  ![](./pictures/21)

  这里使用的协方差方程是 平方指数函数. 

  性质很容易分析, 两者相近则趋近于1; 反之为趋近于0.

  > 这里为什么要用平方指数函数呢? 因为这个在第四章才会将, 不同的模型其实对应着不同的协方差方程. 而线性模型正好对应的就是 平方指数函数, 这里的线性模型的准确英文表达是, Linear-shaped basis function.
  >
  > 而如果模型的函数形状是高斯的话, 其名称为, Gaussian-shaped basis function. 其对应的协方差函数是 **SE 函数**, 形状相当复杂, 有机会看第四章再说这个点.

### 2.3 推导

#### 2.3.1 推导出先验分布

之所以称之为先验分布, 是因为这里的分布是在完全没有数据指导下, 完全依据设计者加入的知识(协方差方程)得到的分布.

###### 1. 采样出n个点

从训练集中得到了采样集 $X_*$, 假设采样了n个点.

###### 2. 得出协方差矩阵

记住每个样本都是一个随机变量, 记下来根据设计的协方差函数, 得到不同随机变量(采样样本)之间的协方差矩阵 $K(X_*,X_*)$, 这个协方差矩阵是 n*n,  n为样本数.

###### 3. 根据正态分布得到预测样本点

上面假设的是每一个X都是一个随机变量, 回顾前面的, 我们知道, $f(x)$ 是遵循正态分布的. 也就是:

![](./pictures/23)

根据这个分布得出预测值. 得出了n个值. 称这n个样本组成的曲线为 **样本曲线**.

下面是图示, 其中蓝色点是采样X对应的真实的Y(由于采样了核方法所以不是线性的), 红色绿色是根据正态分布得到的. **这两条线分别是两次采样采样出来的.** 

> 这里容易搞混的概念是 : 这里的每条线段都是一次采样, 而不是多次采样后构成的曲线. 
>
> 并且这里容易有一个疑问是, 这里的两次采样结果和原先的差距很大啊, 这样的结果不靠谱啊?
>
> 是, 确实不靠谱, 这是因为这里展示的例子的方差较小. 也可以采用较小的方差, 见下一节对positive prefactor的介绍

![](./pictures/24)

###### 4. 关于这个图

- **为何光滑**

  这个图有个很奇怪的性质就是, 他是很光滑的, 但是他的点是从正态分布中采样出来的, 怎么会如此光滑呢?

  平方指数协方差函数是无穷可微的，也就导致了过程是均方根可微的. (这里的解释我完全不懂, 需要4.1节的知识.)

- **特征长度尺度**

  这里还提到了一个特征长度的问题, 特征长度是指对几何体进行长度度量的一个标准, 不同类型的几何体具有不同的值作为特征长度. 例如球体的特征长度就是直径, 线的特征长度就是线. 而在这个里面的特征长度的评价标准理解为 **在input空间移动多少距离后才会使得输出有一个显著的变化** .

  而这个值是可以进行调整的, 这个例子中, 尺度为1. 而我们可以通过 将 $|x_p-x_q|$ 替换为 $|x_p-x_q|/l$,  	$l$ 越大, 得到的样本曲线越平缓.

- **positive prefactor**

  这个是指 **协方差矩阵的系数**, 在这个例子中, 系数是1. 也就是下面这个:

  ![](./pictures/25)

  通过在exp之前加一个正数(应该要小于1, 因为相关系数不能大于1), 可以控制采样出的样本曲线.

#### 2.3.2 推导出后验分布

这里就要结合训练集数据和先验分布进行推导.

###### 1. 得到后验协方差矩阵

这里其实真的就是和上面说的一样, 就是结合起了两者.

![](./pictures/26)

其中, $X$ 是训练集数据中的input, $X_*$ 是测试集数据中的input, 这里通过协方差函数得出了协方差矩阵. 其中, 设训练集中的真实输出是 $f$, 测试集的输出, 也就是我们求的输出为 $f_*$. 

最后的协方差矩阵的大小为 : $(n+n_*)*(n+n_*)$

###### 2. 新旧变量的联合分布

按照前面的2.3的步骤, 这时候理应做的是从2.4.1中制定的协方差矩阵构成的正态分布中采样, 如3中的图, 其本质是一个**联合分布**, 是 **旧变量**$f$ + **新变量**$f_*$ 的联合分布:

![ ](./pictures/27)

这个新的高维高斯分布包含了未知信息, 即, $K(X_*,X)$ ,$K(X,X_*)$ 和 $K(X_*,X_*)$ ,训练集中的输出, 而  $K(X_*,X)$$ ,$$K(X,X_*)$  和 $K(X_*,X_*)$ 的值是可以通过核函数计算出来的.

>   $K(X_*,X)$ 和 $K(X_*,X_*)$ 的计算, 只需要用到 $X_*$ 的值, 不需要计算出对应的 y.

因此, 这个新的高维高斯分布就是我们的目标. 也就是 **后验分布**. 这个分布采样出来的是 $(f,f_*)$

###### 3. 找到有自信的曲线

需要再次注意的是, 这里的曲线(也就是从联合分布中采样出来的值) 对应的是我们要求的分布. (此分布非彼分布, 这里指的是输入为新的数据x是, 输出为y的概率分布)

由这个分布可以采样出很多曲线, 而我们要做的就是不停地采样, 使采样生成的曲线能够包含已知数据, 即训练集数据$f(X)$ , 在下面的图中就是那几个叉叉点. 

简单粗暴的方法就是尝试成千上亿个, 找到符合要求的, 就可以得到下面这样的图.

![](./pictures/28)

> 注意, 这里和上面的图没有关系, 各种颜色的线并没有对应, 都是随机采样出的结果.
>
> 可以从这个新的联合分布中采样出很多的曲线出来, 比如采样了一亿次, 接下来可以通过统计的方法统计出大概的区间. 阴影部分代表的是采样的95%置信区间.

###### 4. 使用解析法求边缘概率

但是, 很明显这种方法是行不通的. 而我们想要的,是把这个采样转换成一个输出只有 $f_*$ 的形式. 也就是 关于 $x_*$ 的边缘概率.

我们可以利用联合高斯分布的consistency性质, 可以转换为下面这个:

![](./pictures/29)

> 具体的转换方法见A.2



### 2.4 Prediction using Noisy Observations

##### 2.4.1 Insight

上面的方法都是认为我们获取的训练集数据是没有噪音的情况的, 但是实际的情况下是有很多噪音的. 因此在这里加入了噪音, 提高模型的 capacity.

表现在上面的图的结果就是, 不会再有叉叉点, 也就是说不会所有的曲线都一定会通过几个点, 会有一个管状的置信区间.

##### 2.4.2 加入噪音

###### 1) 未加入噪音的情况

不再像小孩子一样天真单纯地认为: 

![](./pictures/30)

![](./pictures/33)

![](./pictures/31)

###### 2) 加入噪音的情况

- **对核函数的修改**

  加入了现实的残酷:

  ![](./pictures/34)

  ![](./pictures/32)

  这里的 $\delta_{pq}$ 代表, 如果 p=q 的话,   $\delta_{pq}$ 等于1, 否则等于0. 

  也就是说, 噪音只存在于计算方差而不是协方差的时候, 这个是基于, **变量之间互相独立的** 假设.

- **新的概率方程**

  下面是对于新数据的假设. 这就是 旧随机变量 + 新随机变量 的联合分布.

  ![](./pictures/35)

  而我们要求的是关于 $f_*$ 的分布, 也就是 **边缘概率** , 对这个边缘概率的求解就是:

  ![](./pictures/36)

###### 3) 分析

- **简写方程的解**

  由于每个点之间是相互独立的, 因此可以假设现在只有一个新的点, $x_*$.

  假设一个协方差向量.

  设 $\operatorname{k(x_*)=k_*=\big(k(x_*,x_1),k(x_*,x_2),...,k(x_*,x_n)\big)}$ 

  那么, 上面求解的结果就是:

  ![](./pictures/37)

  这样就得到了边缘概率 $p(f_*)$ 的均值和方差.

- **分析均值**

  上面的求解中, 自变量只有 $\operatorname{k_*}$ , 因此上面的**求均值**式子可以看做是一个**线性组合式.** 右侧的部分都可以看做常数. 即:

  ![](./pictures/38)

  ![](./pictures/39)

  这个结果是符合, representer theorem 的. 

  - 定义:

    a representer theorem is any of several related results stating that a **minimizer** ${\displaystyle{f^{*}}}$ of a **regularized empirical risk function** defined over a **reproducing kernel Hilbert space** can be represented as **a finite linear combination** of kernel products evaluated on the input points in the training set data.

  - 直观的理解是:

    尽管GP在所有的变量上定义了一个联合高斯分布. 

    但是对于每个需要预测的点 $x_*$, 我们都要. 我们唯一关心的是, (n+1)维的分布. 

- **分析方差**

  注意方差的公式中, 没有涉及y值, 这个是高斯分布的特性. 