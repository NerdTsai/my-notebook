## 绪论

### 1.5 决策论

#### 1.5.0 决策与推断

这里针对的是有监督学习.

有监督学习大致分为回归和分类两种情况, 分别对应着连续值和离散值.

这个问题可以进行以下的抽象:

- 条件 :  输入向量 $x=(x_1,x_2,...,x_{t-1})$ 和对应的目标值向量 $t=(t_1,t_2,...,t_{t-1})$
- 目标 : 对于一个新的 $x_t$, 预测新的 $t_t$
- 方法 : 确定联合分布 $P(x,t)$

**推断** : 从训练数据 $(x,t)$ 中推断出 $P(x,t)$

**决策** : 在给定 $x_t$ , 决定 $t_t$ , 这一步通常很简单, 比如最大化 $p(t_t|x_t)$

> 为什么这里的目标是求联合分布$P(x,t)$, 因为可以从联合分布求出以下的任意一项:
>
> $P(x) = \int_tP(x,t)dt$
>
> $P(t) = \int_xP(x,t)dx$
>
> $P(x|t)=P(x,t)/p(t)$
>
> $P(t|x)=P(x,t)/p(x)$

#### 1.5.1 生成模型与辨别模型

##### 1) 生成模型

目的 : 对生成数据的模型进行建模. 

推断与决策:

- 推断 : 目标是利用贝叶斯公式推出最大后验概率,  $P(t|x)$.为了解出这个式子, 需要对 $P(x|t),P(t)$ 都进行建模
- 决策 : 直接利用得到的 $P(t|x)$ 进行推算.

其

##### 2) 辨别模型

目的 : 对条件概率 $P(t|x)$ 建模

推断与决策 :

- 推断 : 直接最大化得出条件概率 $P(t|x)$
- 决策 : 直接利用得到的 $P(t|x)$ 进行推算.



##### 1.5.2 最小化期望损失

我们学习的最终目的就是使得数据能够有最大的可能性被模型包容进去. 

损失的计算是推断的根本, 也就是说, 假设模型已经给定的情况下, 去计算现有数据的发生概率. 

这里的期望是指, 对于不同的错误形式, 可以给予不同的权重. 对于连续的场合比较好理解. 在曲线拟合中, 点离拟合距离的远近就是损失值的一种. 



#### 1.5.3 拒绝选项

对于某些情况, 我们不能接受太低的预测准度, 因此在低概率的地方需要进行人工预测:

![](./pictures/1)



#### 1.5.4 三种学习模型

目标只有一个, 学得 $P(C_k|x)$

##### 1) 生成式模型 (Generative model)

- 推断似然函数 $P(x|C_k)$
- 推断先验函数 $P(C_k)$
- 利用边缘化概率的方法求 $P(x)=\sum_kP(x|C_k)P(C_k)$
- 利用贝叶斯公式求 $P(C_k|x)$

> 优点 : 
>
> 1. $P(C_k)$ 可以简单的通过比例求出
>
> 2. 对于离群点检测 (outlier detection) 很有效. 
>
>    1). 什么是离群点 ?
>
>    离群点指的是特征空间距离数据中心特征空间较远的数据, 对于这些模型而言, 是通过对个数据学习同一个模型, 对于大部分连续可微的模型而言, 相近的数据点之间具有很强的辅助作用, 因此对于一个远离点群的点来说, 是很难预测它的. 对于一个分类问题而言也就是说: $P(x)=\sum_kP(x,C_k)$ 很低. 
>
>    2). 为什么生成式模型可以?
>
>    对于一个离群点而言, 其 $P(x|C_k)$ 可以说是很不准的, 因此其结果要靠先验概率去主导. 
>
> 缺点:
>
> 1. 对于x维度很高的情况, 我们需要大量数据才可以在合理的精度下确定 $P(x|C_k)$ 
>
> 2. 对于学习数据生成的分布的模型而言是有用的, 但是对于简单的分类而言是不必要的. 实验证明, 其不是似然函数中可能包含着后验概率不需要的信息:
>
>    ![](./pictures/2)

##### 2) 辨别式模型 (Discrimative model)

- 直接通过学习学得 $P(C_k|x)$

> 优点 :
>
> - 相比于生成式模型而言, 简单的很多
>
> 缺点:
>
> - 但是还是需要很多数据, 这种情况需要下面的方法

##### 3) 判别函数 (Discrimative function) 

- 直接学得从 x 到 $C_k$ 的映射. 在上面那个图中就是那条绿色的线.

> 优点 : 这种情况, 这需要找出决策边界即可
>
> 缺点 : 不靠谱

