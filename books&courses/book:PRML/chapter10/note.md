## 10 近似推断



### 10.0 Introduction

- **任务描述:**

  在给定可观测变量 $X$ 的情况下, 计算潜在变量Z的后验概率 $p(Z|X)$. 

- **解决方法:**

  - 精确推断 : 推算出后验概率的解析解.
  - 近似推断 : 对后验概率进行近似.

- **精确推断的缺点:**

  - 当潜在空间 $Z$ 维度过高时, 无法估计.
  - 当后验形式过于复杂时, 无法估计.

- **近似推断的种类:**

  - 随机性近似 : 蒙特卡洛方法
  - 确定性近似 : 
    - 变分推断(变分贝叶斯)
    - 期望传播


### 10.1 概率分布的近似方法

#### 10.1.1 Motivation

考虑一些从概率分布中采样的过程. 

若这个过程是非常困难的, 我们该如何办呢?

##### 1) 何为采样困难的分布

- 知道整体的 joint 分布, 但是比较难以将其在计算机上实现采样. 本身随机数生成就是一门很深的学问, 对于一些复杂的分布, 这个会是难上加难.
- 只知道边缘分布, 不知道 joint 分布的情况. 要在这种情况下求出联合概率则势必要求条件概率的边缘分布, 难以解析算得. 

##### 2) 解决方法

Gibbs Sampling



#### 10.1.2 Gibbs sampling

在 Gibbs sampling 中, 对于整体的n维的分布, 采样的是保持 n-m 维的数据不变, 对 剩下的 m 维的条件分布进行采样. 即, 假设知道:
$$
z^{(i)}=(z_1^{(i)}, z_2^{(i)},z_3^{(i)})  \sim p(z_1, z_2, z_3)
$$

> $z_1, z_2, z_3$ 不是结果是采样点的不同维度的值. 

那么就可以通过以下方法对 这个分布 采样.
$$
\begin{align}
z_1^{(i)}&\sim p(z_1|z_2^{(i-1)}, z_3^{(i-1)})\\z_2^{(i)}&\sim p(z_2|z_2^{(i-1)}, z_1^{(i)})\\ z_3^{((i))}&\sim p(z_3|z_2^{(i)}, z_1^{(i)})\\
\end{align}
$$


理想情况如下:

![](./pictures/24)

#### 10.1.3 Blocking Gibbs Sampling  

除了每次只采一个维度的方法之外, 还可以采取一次采多个维度的方法:
$$
\begin{align}
z_1^{(i)}&\sim p(z_1|z_2^{i-1}, z_3^{i-1})\\z_2^{(i)}, z_3^{(i)}&\sim p(z_2,z_3|z_1^{(i)})
\end{align}
$$

#### 10.1.4 Collapsed Gibbs sampling

在某些不需要某些维度信息的情况下, 可以采用这个方法加速采样.

通过对某些轴进行边缘化, 省略对其的采样.
$$
p(z_1,z_2) = \int p(z_1, z_2,z_3)dz_3
$$
得到以下结果:
$$
\begin{align}
z_1^{(i)}&\sim p(z_1|z_2^{i-1})\\z_2^{(i)}&\sim p(z_2|z_1^{(i)})
\end{align}
$$


##### 10.1.5 Gibbs sampling 的问题

我们看到, 这种采样方法需要最开始的一些初始化的点. 而由这些初始化的点扩展到全局就需要一些迭代时间. 这样, 如果初始点不具有一般性, 就会出现下面的情况.

![](./pictures/23)

##### 10.1.6 保证收敛

http://www.cs.cmu.edu/~epxing/Class/10708-14/lectures/lecture17-MCMC.pdf




### 10.2 变分推断-研究问题

#### 10.2.1 泛函的概念

##### 1) 基本概念

输出为函数的映射关系, 例如熵, 输入的是概率分布, 输出的是一个函数:

![](./pictures/1)

##### 2) 泛函的导数

研究的是, 当输入的函数产生无穷小的变化时, 泛函的值, 输出函数的变化情况. 

类似与一般函数的利用求导来求取最优解的方式, 这里可以利用对泛函求导来获取**最优函数**.

这里我们就有了更加具体的目标:

​				***利用对泛函求导来获取最优函数 - 这就是变分***



#### 10.2.2 变分用于推断

使用变分法去推断, 往往使用的是贝叶斯理论. 

> 也就是说, 不同于**微积分**中的使用**函数的导数**的方法去求解. 
>
> **变分法**使用**贝叶斯理论**去求解泛函的导.

##### 1) 定义**推断的问题**:

前面也说到了, 推断的目标是, 找出潜在变量的后验分布 $p(Z|X)$. 

- 定义可观测变量 $X=\{x_1,x_2,...,x_N\}$, 定义潜在变量$Z=\{z_1,z_2,...,z_N\}$
- 假设已经定义两者的联合分布 $P(X,Z)$
- 目标是找到 $p(Z|X)$ , 以及 模型证据因子 $P(X)$ 的分布形式.

##### 2) 定义**变分最优化目标**:

在微积分中的最优化目标是最小数值或者是最大数值.而在变分中, 最优目标是**函数的近似**.

放在贝叶斯中就是**两个分布的距离.**  变分贝叶斯中使用的是 **KL 距离**

##### 3) 融合**问题和目标**:

使用贝叶斯理论:
$$
\ln p(x) = \ln p(z,x)- \ln p(z|x) \\= \ln \frac{p(z,x)}{p(z)} - \ln \frac{p(z|x)}{p(z)}
$$
其中, 我们需要假设, $q(z)$ 和 $\ln \frac{p(z,x)}{p(z)}$ 独立,  $q(z)$ 和 $\ln \frac{p(z|x)}{p(z)}$ 独立. 因此就有了:
$$
\ln p(X) = \ln p(Z,X)- \ln p(Z|X) \\= \ln \frac{p(Z,X)}{q(Z)} - \ln \frac{p(Z|X)}{p(Z)}\\=\int q(Z)\ln \{\frac{p(Z,X)}{q(Z)}\}dZ-\int q(Z)\ln \{\frac{p(Z|X)}{q(Z)}\}dZ
$$
就有了最后的**求解公式**:

![](./pictures/2)

##### 4) 分析公式

**性质** : $\mathcal{L}(q)$ 和 $KL(q||p)$ 都是泛函. 第二个使我们要优化的目标.

**目的 :** 我们最后的公式中包含了 KL距离,  最优化目标就是优化 $q(Z)$, 使得 $p(Z|X)$ 与 $q(Z)$ 完全相同.

**方法 :** 我们最小化的方法并不是直接去求 $KL$ 的最小值, 而是通过最大化 $\mathcal{L}(q)$ 的下界来达到最小化 $KL$ 的目的.

**限制 :** 但是如果是从无限的分布中去找 $q(Z)$ 的话, 基本是不现实的, 因此我们需要限制 $q(Z)$ 的类别.然后寻找这个类别中使得KL散度达到最小值的概率分布. 

##### 5) 对$q(Z)$的限制

即使是限制, 我们也对这个限制的类别有所要求, 要求如下:

- 可以被处理
- 范围充分大, 充分灵活,从而它能够提供对真实后验概率分布的一个足够好的近似。

需要强调的是,施加限制条件的唯一目的是为了计算方便.

**一般的限制方法**就是, 确定潜在变量 Z 的分布形式, 这个分布含有参数 $\omega$ , 这样我们的目标就变成了最优化 $\omega$. 这个方法会在后面讲.

下一节会将一个另外的限制方法 - 分解法



### 10.3 变分推断-分解概率分布

#### 10.3.1 为什么要分解

正如10.1节所讲的, 难以从一个分布中直接采样的时候, 我们采用一个一个维度的方式进行采样.

那么对于近似概率的方法, 由于难以求得其最优解. 我们可以从 Gibbs Sampling 中进行类比.  左边是  Gibbs Sampling 中的概念, 右边是变分中的概念.

- 维度值 $z_i \sim q(z_i)$  
- $(z_1,z_2,z_3) \sim q(z_1,z_2,z_3)$

#### 10.3.2 分解方法

##### 可分解假设

本方法的基本假设 : 限制了 $p(Z)$ 的范围, 假设我们将Z分解成多个不相干分布的乘积形式, 如下:

![](./pictures/3)

注意 : 不需要对每个 $q_i(Z_i)$ 设定一个先验分布.

#### 10.3.3 利用分解转化下界

##### 转化下界 $\mathcal{L}(q)$

这样就可以将 $\mathcal{L}(q)$ 转化为以下的式子, 默认 $q_i(Z_i)=q_i$:

![](./pictures/4)

其中, 第二步的转换**非常重要!!!** 转换思路是暂时只考虑 $q_i$ 的部分. 而将不含有 $q_i$ 的部分化为常数. 变成了一个变量为 $q_i$ 的函数.

其中, 这里定义了一个新的**概率分布** $\tilde{p}(X,Z_j)$, **这个概率分布是相当于一个常数存在的**:

![](./pictures/5)

![](./pictures/6)

为什么要定义一个这样的分布呢? 见下一节.

#### 10.3.4 下界转化为负KL散度

##### 1) 负的KL散度

仔细看转后的下界, 会发现这是一个负的KL散度. 
$$
KL(q_j(Z_j)||\tilde p(X,Z_j)) = -\int q_j(Z_j)\ln\frac{\tilde p(X,Z_j)}{q_j(Z_j)} dZ_i
$$
那么最大化这个下界, 就是最小化 KL 距离, 就是使得 $q_j(Z_j)$ 和 $\tilde p(X,Z_j)$ 的分布趋于一致.

##### 2) KL散度的本质就是分布相等

最小化这个KL距离就是使 $q_j(Z_j)=\tilde{p}(X,Z_j)$

> 也就是将 $q_j(Z_j)$ 用其他的所有 $q_i(Z_i)$ 去表示. 这个非常重要, 构成了迭代求解的条件.

由于 $\tilde p(X,Z_j)$ 是可以计算的, 因此就有了 $q_j(Z_j)$ 的解析解.:
$$
q_j(Z_j) =\tilde p(X,Z_j) \to
$$

$$
\begin{align}
\\\ln q_j(Z_j) &=\ln \tilde p(X,Z_j)\\&=\mathbb{E}_{i\neq j}[\ln p(X,Z)]+const\\&=\int \ln p(X,Z)\prod_{i\neq j}q_i(Z_i)dZ_i + const
\end{align}
$$

##### 3) 最大化下界

最大化下界的过程就是使 $KL\big(q_j(Z_J)||\hat{p}(X,Z_j)  \big)$ 最小的过程. 

那么就是在 **假设** $\{p(z_i)\}_{i\neq j}$ 已知的情况下, 利用下面的公式求出 $q_j(Z_j)$. 

![](./pictures/6.5)

进行归一化:

![](./pictures/7)

#### 10.3.5 真正的近似过程

1. 初始化所有因子的概率 : $\{q_i(Z_i)\}$
2. 利用上面的最大化 $\mathcal{L}(q)$ 的方法, 更新每个因子 $q_i(Z_i)$ 的分布 

> 算法保证收敛, 因为关于每个因子 $q_i(Z_i)$ 是个凸函数. ? 原因没写. 知道就好.

​	

#### 10.3.6 structured variational inference

lei

### 10.4 分解近似的不足

#### 10.4.1 不足所在

正因为这里使用了分解假设, 因此这里的变分推论无法捕捉变量之间的相关信息. 如下图所示:

![](./pictures/12)![](./pictures/13)

对于绿色的目标分布. 红色是模拟分布. 

但是由于无法捕捉相关性. 因此协方差矩阵呈现出对角化. 







